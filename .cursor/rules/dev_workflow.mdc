---
description: Guide for using Task Master to manage task-driven development workflows
globs: **/*
alwaysApply: true
---
# Task Master Development Workflow

This guide outlines the typical process for using Task Master to manage software development projects.

**Imporant**
- Get the value of `global.workingTaskGroup` in `.taskmaster/config.json` file as `currentTaskGroup`, replace all values below when meet `${currentTaskGroup}` below.

## Primary Interaction: MCP Server vs. CLI

Task Master offers two primary ways to interact:

1.  **MCP Server (Recommended for Integrated Tools)**:
    - For AI agents and integrated development environments (like Cursor), interacting via the **MCP server is the preferred method**.
    - The MCP server exposes Task Master functionality through a set of tools (e.g., `get_tasks`, `add_subtask`).
    - This method offers better performance, structured data exchange, and richer error handling compared to CLI parsing.
    - Refer to [`mcp.mdc`](mdc:.cursor/rules/mcp.mdc) for details on the MCP architecture and available tools.
    - A comprehensive list and description of MCP tools and their corresponding CLI commands can be found in [`taskmaster.mdc`](mdc:.cursor/rules/taskmaster.mdc).
    - **Restart the MCP server** if core logic in `scripts/modules` or MCP tool/direct function definitions change.

2.  **`task-master` CLI (For Users & Fallback)**:
    - The global `task-master` command provides a user-friendly interface for direct terminal interaction.
    - It can also serve as a fallback if the MCP server is inaccessible or a specific function isn't exposed via MCP.
    - Install globally with `npm install -g task-master-ai` or use locally via `npx task-master-ai ...`.
    - The CLI commands often mirror the MCP tools (e.g., `task-master list` corresponds to `get_tasks`).
    - Refer to [`taskmaster.mdc`](mdc:.cursor/rules/taskmaster.mdc) for a detailed command reference.

## Standard Development Workflow Process

-   Start new projects by running `initialize_project` tool / `task-master init` or `parse_prd` / `task-master parse-prd --input='<prd-file.txt>'` (see [`taskmaster.mdc`](mdc:.cursor/rules/taskmaster.mdc)) to generate initial tasks.json
    - Optionally, use the `tasks` parameter in `parse_prd` to provide pre-analyzed task arrays instead of generating via LLM: `parse_prd` with `tasks: [...]` (MCP) or `task-master parse-prd --tasks='[...]'` (CLI)
-   Begin coding sessions with `get_tasks` / `task-master list` (see [`taskmaster.mdc`](mdc:.cursor/rules/taskmaster.mdc)) to see current tasks, status, and IDs
-   Determine the next task to work on using `next_task` / `task-master next` (see [`taskmaster.mdc`](mdc:.cursor/rules/taskmaster.mdc)).


-   Select tasks based on dependencies (all marked 'done'), priority level, and ID order
-   Clarify tasks by checking task files in tasks/ directory or asking for user input
-   View specific task details using `get_task` / `task-master show <id>` (see [`taskmaster.mdc`](mdc:.cursor/rules/taskmaster.mdc)) to understand implementation requirements
-   Analyze the task you get and check if you need to break it down to subtasks using `expand_task` / `task-master expand --id=<id> --force --research` (see [`taskmaster.mdc`](mdc:.cursor/rules/taskmaster.mdc)) with appropriate flags like `--force` (to replace existing subtasks) and `--research`.
-   Clear existing subtasks if needed using `clear_subtasks` / `task-master clear-subtasks --id=<id>` (see [`taskmaster.mdc`](mdc:.cursor/rules/taskmaster.mdc)) before regenerating
-   Implement code following task details, dependencies, and project standards
-   Set task status to 'in-progress' using `set_task_status` / `task-master set-status --id=<id> --status=in-progress` when beginning work
-   Move tasks through the configured workflow stages (pending → in-progress → review → done)
-   For AI/agent completed tasks, set status to 'review' with `executor=agent` for initial verification
-   For tasks requiring human validation, move to 'review' status with `executor=human`
-   **CRITICAL**: Follow the Task Verification Workflow (see below) before marking tasks as complete
-   Mark fully completed and verified tasks with `set_task_status` / `task-master set-status --id=<id> --status=done` (see [`taskmaster.mdc`](mdc:.cursor/rules/taskmaster.mdc))
-   Update dependent tasks when implementation differs from original plan using `update` / `task-master update --from=<id> --prompt="..."` or `update_task` / `task-master update-task --id=<id> --prompt="..."` (see [`taskmaster.mdc`](mdc:.cursor/rules/taskmaster.mdc))
-   Add new tasks discovered during implementation using `add_task` / `task-master add-task --prompt="..." --research` (see [`taskmaster.mdc`](mdc:.cursor/rules/taskmaster.mdc)).
-   Add new subtasks as needed using `add_subtask` / `task-master add-subtask --parent=<id> --title="..."` (see [`taskmaster.mdc`](mdc:.cursor/rules/taskmaster.mdc)).
-   Append notes or details to subtasks using `update_subtask` / `task-master update-subtask --id=<subtaskId> --prompt='Add implementation notes here...\nMore details...'` (see [`taskmaster.mdc`](mdc:.cursor/rules/taskmaster.mdc)).
-   Generate task files with `generate` / `task-master generate` (see [`taskmaster.mdc`](mdc:.cursor/rules/taskmaster.mdc)) after updating tasks.json
-   Maintain valid dependency structure with `add_dependency`/`remove_dependency` tools or `task-master add-dependency`/`remove-dependency` commands, `validate_dependencies` / `task-master validate-dependencies`, and `fix_dependencies` / `task-master fix-dependencies` (see [`taskmaster.mdc`](mdc:.cursor/rules/taskmaster.mdc)) when needed
-    Before executing a task, verify all dependency tasks are marked as 'done'. If not, ask the user to complete dependencies first.
-   Respect dependency chains and task priorities when selecting work.
-   Report progress regularly using `get_tasks` / `task-master list`
-   Reorganize tasks as needed using `move_task` / `task-master move --from=<id> --to=<id>` (see [`taskmaster.mdc`](mdc:.cursor/rules/taskmaster.mdc)) to change task hierarchy or ordering

## Task Breakdown Process





## Task Breakdown Process

-   Use `expand_task` / `task-master expand --id=<id>` to break down tasks into subtasks.
-   Use `--num=<number>` to specify an explicit number of subtasks, overriding defaults.
-   Add `--research` flag to leverage Perplexity AI for research-backed expansion.
-   Add `--force` flag to clear existing subtasks before generating new ones (default is to append).
-   Use `--subtasks="<json>"` to provide predefined subtasks directly, bypassing AI generation entirely.
    - Subtasks must follow the required schema with fields: `id`, `title`, `description`, `dependencies`, `details`, `status`
    - Optional fields include: `testStrategy`, `executor`, `verifications`, `results`, `metadata`
    - Example: `--subtasks='[{"id":1,"title":"Setup","description":"Initial setup","dependencies":[],"details":"Create project structure","status":"pending"}]'`
    - When using `--subtasks`, AI generation is skipped, making the operation much faster
-   Review and adjust generated subtasks as necessary.
-   Use `expand_all` tool or `task-master expand --all` to expand multiple pending tasks at once, respecting flags like `--force` and `--research`.
-   If subtasks need complete replacement (regardless of the `--force` flag on `expand`), clear them first with `clear_subtasks` / `task-master clear-subtasks --id=<id>`.

## Task Verification Workflow

**CRITICAL RULE**: Before executing any task, you MUST follow this verification workflow to ensure task completion is properly validated.

### Verification Process Overview

1. **Check Existing Verifications**: Examine the task's `verifications` array
2. **Verify Each Item**: Test each verification to determine if it passes
3. **Update Verification Status**: Mark verifications as `passed: true` or `passed: false`
4. **Execute Task Only If Needed**: Only implement the task if verifications fail

### Detailed Verification Steps

#### Step 1: Examine Task Verifications
```javascript
// Example task structure with verifications
{
  "id": 5,
  "title": "Setup API Authentication",
  "verifications": [
    {
      "description": "API endpoint /auth/login exists and responds",
      "passed": false
    },
    {
      "description": "Authentication middleware is properly configured",
      "passed": false
    },
    {
      "description": "JWT token generation works correctly",
      "passed": false
    }
  ]
}
```

#### Step 2: Verify Each Item Systematically

**For each verification in the task's `verifications` array:**

1. **Read the verification description carefully**
2. **Test the specific condition described**
3. **Determine if the verification passes based on factual evidence**
4. **Update the verification status accordingly**

**Verification Testing Methods:**
- **Code Inspection**: Check if files, functions, or configurations exist
- **Functional Testing**: Test endpoints, run code, verify behavior
- **File System Checks**: Verify files exist in expected locations
- **Configuration Validation**: Check settings and environment variables
- **Integration Testing**: Verify components work together

#### Step 3: Update Verification Status

Use `update_task` or `update_subtask` to update verification status:

```bash
# Example: Update task verifications after testing
task-master update-task --id=5 --prompt="Verification Results:
1. API endpoint /auth/login - PASSED: Endpoint exists and returns 200
2. Authentication middleware - FAILED: Middleware not configured
3. JWT token generation - FAILED: No JWT implementation found

Updated verifications array with current status."
```

#### Step 4: Decision Matrix

**If ALL verifications pass (`passed: true`):**
- ✅ **DO**: Update task results with verification summary
- ✅ **DO**: Apply automatic status progression based on `defaultFlow`
- ✅ **DO**: Read `.taskmaster/${currentTaskGroup}/config.json` and progress to next state
- ❌ **DON'T**: Implement the task (it's already complete)

**If ANY verifications fail (`passed: false`):**
- ✅ **DO**: Proceed with task implementation
- ✅ **DO**: Focus implementation on failing verifications
- ✅ **DO**: Re-verify after implementation
- ✅ **DO**: Update verification status to `passed: true` when fixed
- ✅ **DO**: Apply automatic status progression after successful implementation

**If verifications array is empty or missing:**
- ✅ **DO**: Use the `testStrategy` field as verification guidance
- ✅ **DO**: Follow normal implementation process
- ✅ **DO**: Apply automatic status progression after task completion

### Verification Examples

#### Example 1: All Verifications Pass
```bash
# Check task verifications
task-master show 5

# Test each verification
# 1. Check API endpoint: curl http://localhost:3000/auth/login ✅
# 2. Check middleware: grep -r "authMiddleware" src/ ✅  
# 3. Check JWT: npm list jsonwebtoken ✅

# Update task with results
task-master update-task --id=5 --prompt="All verifications confirmed:
- API endpoint responds correctly
- Authentication middleware found and configured  
- JWT implementation verified
Task is complete, no implementation needed."

# Apply automatic status progression based on defaultFlow
# (Read config.json, find current state, progress to next state)
# Example: from "in-progress" -> "review" based on defaultFlow
task-master set-status --id=5 --status=review --executor=agent
```

#### Example 2: Some Verifications Fail
```bash
# Check task verifications  
task-master show 8

# Test each verification
# 1. Database connection: ❌ Connection fails
# 2. User model exists: ✅ Model found
# 3. Migration files: ❌ No migrations

# Update verification status
task-master update-task --id=8 --prompt="Verification Results:
1. Database connection - FAILED: Connection timeout
2. User model exists - PASSED: Found in models/User.js
3. Migration files - FAILED: No migration directory found

Need to implement database setup and migrations."

# Proceed with implementation focusing on failed items
# ... implement database connection and migrations ...

# Re-verify after implementation
# Update verifications to passed: true when complete

# Apply automatic status progression after successful implementation
# Read defaultFlow from config.json and progress to next state
task-master set-status --id=8 --status=review --executor=agent
```

### Verification Best Practices

#### Creating Effective Verifications
- ✅ **DO**: Write specific, testable verification criteria
- ✅ **DO**: Use factual, observable conditions
- ✅ **DO**: Include both positive and negative test cases
- ❌ **DON'T**: Write vague or subjective verifications
- ❌ **DON'T**: Create verifications that can't be objectively tested

#### Good Verification Examples
```javascript
// ✅ GOOD: Specific and testable
{
  "description": "HTTP server starts on port 3000 without errors",
  "passed": false
}

// ✅ GOOD: File-based verification
{
  "description": "Configuration file config/database.js exists with valid JSON",
  "passed": false  
}

// ✅ GOOD: Functional verification
{
  "description": "User registration endpoint returns 201 status for valid input",
  "passed": false
}
```

#### Poor Verification Examples
```javascript
// ❌ BAD: Too vague
{
  "description": "System works correctly",
  "passed": false
}

// ❌ BAD: Subjective
{
  "description": "Code quality is good",
  "passed": false
}

// ❌ BAD: Not testable
{
  "description": "Users will be happy with the interface",
  "passed": false
}
```

### Integration with Task Status

**Status Progression with Verifications:**

1. **`pending`** → Run verification workflow
2. **`in-progress`** → Implement only if verifications fail
3. **`review`** → Re-verify all items after implementation
4. **`done`** → All verifications must pass

**Verification Commands:**
- Use `get_task` / `task-master show <id>` to view current verifications
- Use `update_task` / `task-master update-task --id=<id>` to update verification status
- Use `update_subtask` / `task-master update-subtask --id=<id>` for subtask verifications

### Automation Integration

**For AI Agents:**
- Always check verifications before starting implementation
- Update verification status based on actual testing results
- Document verification process in task results
- Only implement when verifications demonstrate need

**For Human Developers:**
- Use verifications as acceptance criteria
- Test each verification manually or with automated tests
- Update task with verification results before marking complete

This verification workflow ensures that tasks are only implemented when actually needed and that completion is based on objective, testable criteria rather than assumptions.

## Implementation Drift Handling

-   When implementation differs significantly from planned approach
-   When future tasks need modification due to current implementation choices
-   When new dependencies or requirements emerge
-   Use `update` / `task-master update --from=<futureTaskId> --prompt='<explanation>\nUpdate context...' --research` to update multiple future tasks.
-   Use `update_task` / `task-master update-task --id=<taskId> --prompt='<explanation>\nUpdate context...' --research` to update a single specific task.

## Task Status Management

### Default Task Flow

Taskmaster supports configurable task flow states through the `defaultFlow` setting in `.taskmaster/${currentTaskGroup}/config.json`.

**CRITICAL RULE - Automatic Status Progression After Task Execution:**

After completing any task implementation or execution, you MUST automatically progress the task status according to the `defaultFlow` configuration:

1. **Read Configuration**: Load the current task group's `.taskmaster/${currentTaskGroup}/config.json` file
2. **Find Current State**: Locate the current task's `status` and `executor` in the `defaultFlow` array
3. **Progress to Next State**: Update the task to the next status/executor combination in the flow
4. **Apply Update**: Use `set_task_status` / `task-master set-status` to apply the progression

**Implementation Pattern:**
```bash
# After completing task implementation/execution:
# 1. Read the defaultFlow from config
# 2. Find current state: {status: "in-progress", executor: "agent"}
# 3. Progress to next state: {status: "review", executor: "agent"}
# 4. Update the task
task-master set-status --id=<taskId> --status=review --executor=agent
```

**Matching Rules:**
- `*` in defaultFlow matches any value (wildcard)
- Both `status` and `executor` must match the current task state
- Progress to the immediate next item in the defaultFlow array
- If no match found or at end of flow, keep current status

**Example defaultFlow Configuration:**
```json
{
  "defaultFlow": [
    {"status": "pending", "executor": "*"},
    {"status": "in-progress", "executor": "agent"},
    {"status": "review", "executor": "agent"},
    {"status": "done", "executor": "*"}
  ]
}
```

**When to Apply:**
- ✅ **DO**: Apply after completing task implementation
- ✅ **DO**: Apply after verifying all task verifications pass
- ✅ **DO**: Apply after successful subtask completion
- ✅ **DO**: Apply when transitioning from any workflow state
- ❌ **DON'T**: Apply if task is already at the final state in defaultFlow
- ❌ **DON'T**: Apply if no matching state found in defaultFlow

### Status Usage Guidelines

-   **'pending'**: New tasks or tasks with all dependencies satisfied, ready for work
-   **'in-progress'**: Tasks actively being implemented (limit to 1-2 tasks per developer)
-   **'review'**: Tasks completed and awaiting verification. Use `executor` field to specify reviewer type:
    - Tasks with `status="review"` and `executor="agent"` are completed by AI/agent awaiting verification
    - Tasks with `status="review"` and `executor="human"` require manual validation, testing, or approval
-   **'done'**: Fully completed, tested, and accepted tasks
-   **'deferred'**: Postponed tasks (not part of active flow)
-   **Custom statuses**: Add project-specific workflow states as needed (e.g., 'blocked', 'testing', 'deployed')

## Task Structure Fields

- **id**: Unique identifier for the task (Example: `1`, `1.1`)
- **title**: Brief, descriptive title (Example: `"Initialize Repo"`)
- **description**: Concise summary of what the task involves (Example: `"Create a new repository, set up initial structure."`)
- **status**: Current state of the task (Example: `"pending"`, `"done"`, `"deferred"`)
- **dependencies**: IDs of prerequisite tasks (Example: `[1, 2.1]`)
    - Dependencies are displayed with status indicators (✅ for completed, ⏱️ for pending)
    - This helps quickly identify which prerequisite tasks are blocking work
- **priority**: Importance level (Example: `"high"`, `"medium"`, `"low"`)
- **details**: In-depth implementation instructions (Example: `"Use GitHub client ID/secret, handle callback, set session token."`) 
- **testStrategy**: Verification approach (Example: `"Deploy and call endpoint to confirm 'Hello World' response."`) 
- **executor**: Who should execute this task (Example: `"agent"`, `"human"`)
    - `"agent"`: Task should be executed by AI agents automatically
    - `"human"`: Task requires manual execution by humans and should not be auto-executed by LLMs
    - Defaults to `"agent"` if not specified
- **subtasks**: List of smaller, more specific tasks (Example: `[{"id": 1, "title": "Configure OAuth", ...}]`) 
- **verifications**: Array of verification objects for AI task completion validation (Example: `[{"description": "API endpoint responds", "passed": true}]`)
- **results**: String field for AI to update with task execution results (Example: `"Successfully implemented OAuth flow"`)
- **metadata**: Complex object containing custom fields, MCP servers, and task group relationships (Example: `{"fields": {...}, "mcpServers": [...], "taskGroups": [...]}`)
- Refer to task structure details (previously linked to `tasks.mdc`).

## Configuration Management (Updated)

Taskmaster configuration is managed through two main mechanisms:

1.  **`.taskmaster/config.json` File (Primary):**
    *   Located in the project root directory.
    *   Stores most configuration settings: AI model selections (main, research, fallback), parameters (max tokens, temperature), logging level, default subtasks/priority, project name, defaultFlow workflow states, etc.
    *   **Managed via `task-master models --setup` command.** Do not edit manually unless you know what you are doing.
    *   **View/Set specific models via `task-master models` command or `models` MCP tool.**
    *   **Configure workflow states** by editing the `defaultFlow` array in the `global` section.
    *   Created automatically when you run `task-master models --setup` for the first time.

2.  **Environment Variables (`.env` / `mcp.json`):**
    *   Used **only** for sensitive API keys and specific endpoint URLs.
    *   Place API keys (one per provider) in a `.env` file in the project root for CLI usage.
    *   For MCP/Cursor integration, configure these keys in the `env` section of `.cursor/mcp.json`.
    *   Available keys/variables: See `assets/env.example` or the Configuration section in the command reference (previously linked to `taskmaster.mdc`).

**Important:** Non-API key settings (like model selections, `MAX_TOKENS`, `TASKMASTER_LOG_LEVEL`) are **no longer configured via environment variables**. Use the `task-master models` command (or `--setup` for interactive configuration) or the `models` MCP tool.
**If AI commands FAIL in MCP** verify that the API key for the selected provider is present in the `env` section of `.cursor/mcp.json`.
**If AI commands FAIL in CLI** verify that the API key for the selected provider is present in the `.env` file in the root of the project.

## Determining the Next Task

- Run `next_task` / `task-master next` to show the next task to work on.
- The command identifies tasks with all dependencies satisfied and appropriate status
- Tasks are prioritized by priority level, dependency count, and ID
- Consider the configured workflow states when selecting tasks:
  - **'pending'** tasks are ready for implementation
  - **'in-progress'** tasks are currently being worked on (limit active work)
  - **'review'** tasks await verification before completion (check `executor` field for reviewer type)
  - **'done'** tasks are complete and can satisfy dependencies for other tasks
- The command shows comprehensive task information including:
    - Basic task details and description
    - Implementation details
    - Subtasks (if they exist)
    - Contextual suggested actions
- Recommended before starting any new development work
- Respects your project's dependency structure
- Ensures tasks are completed in the appropriate sequence
- Provides ready-to-use commands for common task actions

## Viewing Specific Task Details

- Run `get_task` / `task-master show <id>` to view a specific task.
- Use dot notation for subtasks: `task-master show 1.2` (shows subtask 2 of task 1)
- Displays comprehensive information similar to the next command, but for a specific task
- For parent tasks, shows all subtasks and their current status
- For subtasks, shows parent task information and relationship
- Provides contextual suggested actions appropriate for the specific task
- Useful for examining task details before implementation or checking status

## Managing Task Dependencies

- Use `add_dependency` / `task-master add-dependency --id=<id> --depends-on=<id>` to add a dependency.
- Use `remove_dependency` / `task-master remove-dependency --id=<id> --depends-on=<id>` to remove a dependency.
- The system prevents circular dependencies and duplicate dependency entries
- Dependencies are checked for existence before being added or removed
- Task files are automatically regenerated after dependency changes
- Dependencies are visualized with status indicators in task listings and files

## Task Reorganization

- Use `move_task` / `task-master move --from=<id> --to=<id>` to move tasks or subtasks within the hierarchy
- This command supports several use cases:
  - Moving a standalone task to become a subtask (e.g., `--from=5 --to=7`)
  - Moving a subtask to become a standalone task (e.g., `--from=5.2 --to=7`) 
  - Moving a subtask to a different parent (e.g., `--from=5.2 --to=7.3`)
  - Reordering subtasks within the same parent (e.g., `--from=5.2 --to=5.4`)
  - Moving a task to a new, non-existent ID position (e.g., `--from=5 --to=25`)
  - Moving multiple tasks at once using comma-separated IDs (e.g., `--from=10,11,12 --to=16,17,18`)
- The system includes validation to prevent data loss:
  - Allows moving to non-existent IDs by creating placeholder tasks
  - Prevents moving to existing task IDs that have content (to avoid overwriting)
  - Validates source tasks exist before attempting to move them
- The system maintains proper parent-child relationships and dependency integrity
- Task files are automatically regenerated after the move operation
- This provides greater flexibility in organizing and refining your task structure as project understanding evolves
- This is especially useful when dealing with potential merge conflicts arising from teams creating tasks on separate branches. Solve these conflicts very easily by moving your tasks and keeping theirs.

## Iterative Subtask Implementation

Once a task has been broken down into subtasks using `expand_task` or similar methods, follow this iterative process for implementation:

1.  **Understand the Goal (Preparation):**
    *   Use `get_task` / `task-master show <subtaskId>` (see [`taskmaster.mdc`](mdc:.cursor/rules/taskmaster.mdc)) to thoroughly understand the specific goals and requirements of the subtask.

2.  **Initial Exploration & Planning (Iteration 1):**
    *   This is the first attempt at creating a concrete implementation plan.
    *   Explore the codebase to identify the precise files, functions, and even specific lines of code that will need modification.
    *   Determine the intended code changes (diffs) and their locations.
    *   Gather *all* relevant details from this exploration phase.

3.  **Log the Plan:**
    *   Run `update_subtask` / `task-master update-subtask --id=<subtaskId> --prompt='<detailed plan>'`.
    *   Provide the *complete and detailed* findings from the exploration phase in the prompt. Include file paths, line numbers, proposed diffs, reasoning, and any potential challenges identified. Do not omit details. The goal is to create a rich, timestamped log within the subtask's `details`.

4.  **Verify the Plan:**
    *   Run `get_task` / `task-master show <subtaskId>` again to confirm that the detailed implementation plan has been successfully appended to the subtask's details.

5.  **Begin Implementation:**
    *   Set the subtask status using `set_task_status` / `task-master set-status --id=<subtaskId> --status=in-progress`.
    *   Start coding based on the logged plan.

6.  **Refine and Log Progress (Iteration 2+):**
    *   As implementation progresses, you will encounter challenges, discover nuances, or confirm successful approaches.
    *   **Before appending new information**: Briefly review the *existing* details logged in the subtask (using `get_task` or recalling from context) to ensure the update adds fresh insights and avoids redundancy.
    *   **Regularly** use `update_subtask` / `task-master update-subtask --id=<subtaskId> --prompt='<update details>\n- What worked...\n- What didn't work...'` to append new findings.
    *   **Crucially, log:**
        *   What worked ("fundamental truths" discovered).
        *   What didn't work and why (to avoid repeating mistakes).
        *   Specific code snippets or configurations that were successful.
        *   Decisions made, especially if confirmed with user input.
        *   Any deviations from the initial plan and the reasoning.
    *   The objective is to continuously enrich the subtask's details, creating a log of the implementation journey that helps the AI (and human developers) learn, adapt, and avoid repeating errors.

7.  **Review & Update Rules (Post-Implementation):**
    *   Once the implementation for the subtask is functionally complete, review all code changes and the relevant chat history.
    *   Identify any new or modified code patterns, conventions, or best practices established during the implementation.
    *   Create new or update existing rules following internal guidelines (previously linked to `cursor_rules.mdc` and `self_improve.mdc`).

8.  **Progress Through Workflow:**
    *   **CRITICAL**: After completing implementation, apply automatic status progression based on `defaultFlow`
    *   Read the current task group's `.taskmaster/${currentTaskGroup}/config.json` file to get the `defaultFlow` configuration
    *   Find the current task state (`status` and `executor`) in the `defaultFlow` array
    *   Progress to the next state in the flow using: `set_task_status` / `task-master set-status --id=<subtaskId> --status={nextStatus} --executor={nextExecutor}`
    *   Example progression: `in-progress` + `agent` → `review` + `agent` → `done` + `*`

9.  **Commit Changes (If using Git):**
    *   Stage the relevant code changes and any updated/new rule files (`git add .`).
    *   Craft a comprehensive Git commit message summarizing the work done for the subtask, including both code implementation and any rule adjustments.
    *   Execute the commit command directly in the terminal (e.g., `git commit -m 'feat(module): Implement feature X for subtask <subtaskId>\n\n- Details about changes...\n- Updated rule Y for pattern Z'`).

10. **Proceed to Next Subtask:**
    *   Identify the next subtask (e.g., using `next_task` / `task-master next`).

## Code Analysis & Refactoring Techniques

- **Top-Level Function Search**:
    - Useful for understanding module structure or planning refactors.
    - Use grep/ripgrep to find exported functions/constants:
      `rg "export (async function|function|const) \w+"` or similar patterns.
    - Can help compare functions between files during migrations or identify potential naming conflicts.

---
*This workflow provides a general guideline. Adapt it based on your specific project needs and team practices.*